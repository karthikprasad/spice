\documentclass[letterpaper]{article}
\usepackage{proceed2e}
\usepackage[margin=1in]{geometry}

% Set the typeface to Times Roman
\usepackage{times}
\usepackage{graphicx}

\title{Discrete Time Discrete Symbol Sequence Prediction Using HMM}

\author{Zhengli Zhao, Karthik Prasad, Abhisaar Sharma} 

\begin{document}

\maketitle

\begin{abstract}
This paper describes our graphical-model approach to the data-oriented project “SPiCe” (The Sequence PredictIction ChallengE). Hidden Markov model (HMM) is a statistical Markov model in which the system being modelled is assumed to be a Markov process with latent states and can be presented as the simplest Bayesian network. We show our approach in modelling the problem as a HMM and using learning techniques like Baum Welch algorithm and Spectral learning methods to learn the parameters of this graphical model and make predictions.

\end{abstract}

\section{Introduction}
The Sequence PredictIction ChallengE (SPiCe) is a competition where the aim is to learn a model that allows the ranking of potential next symbols for a given prefix and submitting a ranking of the 5 most probable next symbols. Training datasets consist of variable length sequences with a fixed number of symbols. The competition uses real-world data from different fields like Natural Language Processing, Biology, Signal Processing, Software Verification.

\begin{figure}[h]
\includegraphics[scale=0.2]{"hmm"}
\caption{A hidden Markov model}
\end{figure}

We can use a Hidden Markov Model to model this problem, where the training sequences can be treated as discrete time observables (emission variables) and the unobserved latent states can be used to capture the intrinsic sequence structure. A hidden Markov model can be considered a generalization of a mixture model where the hidden variables which control the mixture component to be selected for each observation, are related through a Markov process rather than independent of each other. 


\section{Methodology}

While trying to model the training sequence as an HMM, we would like to learn the parameters of this graphical model. We will present two approaches that we explored, the first one is the Baum–Welch algorithm which uses the EM algorithm to find the maximum likelihood estimate of the parameters of a hidden Markov model given a set of observed feature vectors and the second one is using spectral methods.

\subsection{Baum-Welch Algorithm}

The Baum–Welch algorithm is used to find the unknown parameters of a hidden Markov model (HMM). It makes use of the forward-backward algorithm. Let $X_{t}$ be a discrete hidden random variable with $N$ possible values. We assume the $P(X_{t}|X_{t-1})$ is independent of time t. Let the state transitions be described by $A$ which is a homogeneous time independent stochastic transition matrix. 
\begin{center}
$A = \left\lbrace a_{ij} \right\rbrace = P(X_{t}=j|X_{t-1}=i)$
\end{center}
The initial state distribution is given by
\begin{center}
$\pi_{i} = P(X_{1}=i)$
\end{center}
Let the observation variables be $Y_{t}$ which can take one of $K$ possible values. Let $B$ describe the probability of a certain observation at time $t$ for a state $j$. The emission matrix is then defined as
\begin{center}
$B = \left\lbrace b_{j}(y_{t}) \right\rbrace = P(Y_{t}=y_{t}|X_{t}=j)$
\end{center}
A single observable sequence is given by 
\begin{center}
$Y = (Y_{1}=y_{1},Y_{2}=y_{2},Y_{3}=y_{3},..,Y_{T}=y_{t})$
\end{center}

A Hidden Markov Model is completely parametrized by  $\theta = (A,B,\pi)$. The Baum–Welch algorithm finds a local maximum for $\theta^{*} = argmax_{\theta} P(Y|\theta)$, the HMM parameters $\theta$ that maximise the probability of the given observation sequence.
 
For finding the locally optimum $\theta$, it is randomly initialized as $(A, B, \pi)$. Since it has EM type updates, the Expectation step involves finding the forward and backward probabilities, which uses a dynamic programming algorithm to find the most likely states. The Maximization step then uses these probability values to estimate the parameters $\theta$ of the HMM.

\subsubsection{Forward Procedure}
Let $\alpha_{i}(t)$ =$P(Y_{1}=y_{1},Y_{2}=y_{2},...,Y_{t}=y_{t},X_{t}={i}|\theta)$, which is the probability of seeing the output sequence $y_{1},y_{2},...,y_{t}$ and being in state $i$ at time $t$. This is found recursively by the following equations:

\begin{center}
$\alpha_{i}(1)=\pi_{i} b_{i}(y_{1})$

$\alpha_{j}(t+1)=b_{j}(y_{t+1}) \sum_{i=1}^{N}\alpha_{i}(t) a_{ij}$
\end{center}

\subsubsection{Backward Procedure}

Let $\beta_{i}(t)=P(Y_{t+1}=y_{t+1},...,Y_{T}=y_{T}|X_{t}=i,\theta)$, which is the probability of the ending partial sequence $y_{t+1},...,y_{T}$ given the starting state $i$ at time $t$. $\beta_{i}(t)$ is found recursively by:
\begin{center}
$\beta_{i}(T)=1$


$\beta_{i}(t)=\sum_{j=1}^N \beta_{j}(t+1) a_{ij} b_{j}(y_{t+1})$
\end{center}
 
One problem is that in case of long sequences as required by the Sequence prediction challenge, the forward probability values can go to zero exponentially. The solution to this problem is provided in the implementation section by using a normalized version of the Forward Backward procedure.

\subsubsection{Updates}

To form the updates, let us define $\gamma_{i}(t)$ which will be the probability of being in a state $i$ at time $t$ given the observed sequence $Y$ and the parameters $\theta$. It can be shown that

$
\gamma_{i}(t)=P(X_{t}=i|Y,\theta) = \frac{\alpha_{i}(t)\beta_{i}(t)}{\sum_{j=1}^N \alpha_{j}(t)\beta_{j}(t)}
$

Lets define $\xi_{ij}(t)$ as the probability of being in state $i$ and $j$ at times $t$ and $t+1$ respectively given the observed sequence $Y$ and parameters $\theta$ which can be shown to be equal to

\begin{center}

$\xi_{ij}(t)=P(X_{t}=i,X_{t+1}=j|Y,\theta)$

$\xi_{ij}(t)=\frac{\alpha_{i}(t) a_{ij} \beta_{j}(t+1) b_{j}(y_{t+1})}{\sum_{k=1}^N \alpha_k(T)}$
\end{center}

$\theta$ is updated with the following set of equations:

\begin{center}
$\pi_{i}^* = \gamma_{i}(1)$
\end{center}

which is the probability of state $i$ at time $1$.

\begin{center}
$a_{ij}^*=\frac{\sum^{T-1}_{t=1}\xi_{ij}(t)}{\sum^{T-1}_{t=1}\gamma_{i}(t)}$
\end{center}

which is the expected number of transitions from state $i$ to state $j$ compared to the expected total number of transitions away from state $i$.

\begin{center}
$b_{i}^*(v_{k})=\frac{\sum^T_{t=1,o_{t}=v_{k}} \gamma_{i}(t)}{\sum^T_{t=1} \gamma_i(t)}$
\end{center}

$b_{i}^*(v_{k})$ is the expected number of times the output observations have been equal to $v_{k}$ while in state $i$ over the expected total number of times in state $i$. The updates are done until a desired level of convergence.

\subsection{Spectral learning}

\section{Implementation}

\subsubsection{Baum-Welch Algorithm}

The Baum-Welch algorithm was written in Java language. Python was initially used but dropped since it had extremely slow performance\footnote{Baum-Welch speed comparison: http://www.math.univ-toulouse.fr/~agarivie/Telecom/code/index.php}. The following points describe the sequence of optimizations we did.

\begin{itemize}
\item Hmm learn\footnote{https://github.com/hmmlearn/hmmlearn} - a python based library which consisted algorithms for unsupervised learning and inference of Hidden Markov Models was first used to evaluate the accuracy of this method on the data sets. Hmm learn is also included in scikit, however this library took a lot of time learning the model using Baum-Welch. The number of input sequences were at least 20,000 for each problem and the number of hidden states was at least 4. For the smallest problem it took more than one day to train the model. For larger problems, the code took about 4 days to run and exited with out of memory errors - this library was not very useful for our problem.

\item Due to the inefficient library, we implemented the Baum-Welch algorithm in Java. Since the number of observed sequences is large, we implemented a mini-batch type learning of Baum-Welch algorithm. The input sequences were broken into batches (about 10-50 per batch) and the HMM parameters were learned for a given batch. For the next mini-batch the HMM was initialized with $\theta$ obtained from the previous batch.

\item To get more stable and accurate solutions, we decreased contribution of the weight learned by later mini-batches by using weight smoothing.

\begin{center}
$\Theta_{n} = (1-\alpha_{n})\Theta_{n-1} + \alpha_{n} \theta_{n}$
\end{center}

Where $\alpha_{n}$ is $1/n$, $n$ is the number of the mini-batch. $\Theta_{n}$ is the overall weight of the HMM, $\theta_{n}$ is the locally optimal set of weights returned by Baum-Welch algorithm initialized from $\Theta_{n-1}$ 


\item For longer sequences, Baum-Welch algorithm is hard to use because the forward probability values quickly became very small and go out of range of float and double data-types. This leads to underflow problems in forward-backward algorithm. We tried solving the problem by using BigDecimal class of java which does exact arithmetic, but it was too slow could not be used. Another approach we tried was transforming the probabilities into corresponding logs, but there are terms in Baum-Welch which would then need computation of sum inside logs, hence this approach was not very useful.   

\item Baum-Welch was then modified  to prevent underflow, the idea is to normalize $\alpha_t(i)$ so that $\hat{\alpha}_{t}(i)$ the normalized $\hat{\alpha}_{t}(i)$, would be proportional to $\alpha_{t}(i)$ and sum to 1 over all possible states. We can calculate the normalizers using the following equations. 

\begin{center}

$\sum_{i=1}^{N}\hat{\alpha}_{i}(t) = 1 , \hat{\alpha}_{t}(i) = \prod_{k=1}^t\eta_{k}\alpha_{t}(i)$ 

$\prod_{k=1}^t\eta_{k}\alpha_{t}(i) = 1/\sum_{i=1}^N\alpha_{t}(i)$

\end{center}

In the normalized Baum-Welch algorithm, we do a normalization at each step using the constants $\eta_{t}$ for both the forward and backward steps for all the values. The updates are done in using the regular formulae as described earlier.
 
\end{itemize}

\subsubsection{Spectral learning}



\section{Results}

Our current competition rank is number 3!. Thanks to Zhengli's untiring efforts and unparalleled intelligence$\longrightarrow\infty$:D. 




\subsubsection{Citations in Text}

Citations within the text should include the author's last name and
year, e.g., (Cheesman, 1985). Reference style should follow the style
that you are used to using, as long as the citation style is
consistent.

For the original submission, take care not to reveal the authors' identity through
the manner in which one's own previous work is cited.  For example, writing
``In (Bovik, 1970), we studied the problem of AI'' would be inappropriate, as
it reveals the author's identity.  Instead, write ``(Bovik, 1970) studied the
problem of AI.''

\subsubsection{Footnotes}

Indicate footnotes with a number\footnote{Sample of the first
footnote} in the text. Use 8 point type for footnotes.  Place the
footnotes at the bottom of the page on which they appear.  Precede the
footnote with a 0.5 point horizontal rule 1~inch (6~picas)
long.\footnote{Sample of the second footnote}



\subsubsection{Tables}

All tables must be centered, neat, clean, and legible. Table number
and title always appear above the table.  See
Table~\ref{sample-table}.

One line space before the table title, one line space after the table
title, and one line space after the table. The table title must be
initial caps and each table numbered consecutively.

\begin{table}[h]
\caption{Sample Table Title}
\label{sample-table}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION} \\
\hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}

\newpage


\subsubsection*{References}

References follow the acknowledgements.  Use unnumbered third level
heading for the references title.  Any choice of citation style is
acceptable as long as you are consistent.


J.~Alspector, B.~Gupta, and R.~B.~Allen  (1989). Performance of a
stochastic learning microchip.  In D. S. Touretzky (ed.), {\it Advances
in Neural Information Processing Systems 1}, 748-760.  San Mateo, Calif.:
Morgan Kaufmann.

F.~Rosenblatt (1962). {\it Principles of Neurodynamics.} Washington,
D.C.: Spartan Books.

G.~Tesauro (1989). Neurogammon wins computer Olympiad.  {\it Neural
Computation} {\bf 1}(3):321-323.

\end{document}
